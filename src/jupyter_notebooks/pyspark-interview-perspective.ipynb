{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71627bce-abc1-4c6f-bdd8-078cabf6aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing basic libraries\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed460ed-0407-42ba-8926-ca2d8ae15f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating spark session object\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3131116e-0acd-4479-925b-64e08a893f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |name |\n",
      "+---+-----+\n",
      "|1  |Alice|\n",
      "|2  |Bob  |\n",
      "+---+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "df1 partition size 4\n"
     ]
    }
   ],
   "source": [
    "# Sample data for the first DataFrame\n",
    "data1 = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Cathy\"),\n",
    "    (4, \"David\"),\n",
    "    (5, \"Eva\"),\n",
    "    (6, \"Frank\"),\n",
    "    (7, \"Grace\"),\n",
    "    (8, \"Henry\"),\n",
    "    (9, \"Ivy\"),\n",
    "    (10, \"Jack\"),\n",
    "]\n",
    "\n",
    "# Columns for the first DataFrame\n",
    "columns1 = [\"id\", \"name\"]\n",
    "\n",
    "# Creating the first DataFrame\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "\n",
    "# Show the first DataFrame\n",
    "df1.show(2,False)\n",
    "\n",
    "print(f\"df1 partition size {df1.rdd.getNumPartitions()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bdc15c7-8e96-47af-8450-6ca6141673c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "|id |subject|score|\n",
      "+---+-------+-----+\n",
      "|1  |Math   |85   |\n",
      "|2  |English|78   |\n",
      "+---+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "df2 partition size 4\n"
     ]
    }
   ],
   "source": [
    "# Sample data for the second DataFrame\n",
    "data2 = [\n",
    "    (1, \"Math\", 85),\n",
    "    (2, \"English\", 78),\n",
    "    (3, \"History\", 90),\n",
    "    (4, \"Science\", 88),\n",
    "    (5, \"Geography\", 76),\n",
    "    (6, \"Art\", 95),\n",
    "    (7, \"Music\", 80),\n",
    "    (8, \"Physical Education\", 92),\n",
    "    (9, \"Computer Science\", 89),\n",
    "    (10, \"Biology\", 91),\n",
    "]\n",
    "\n",
    "# Columns for the second DataFrame\n",
    "columns2 = [\"id\", \"subject\", \"score\"]\n",
    "\n",
    "# Creating the second DataFrame\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Show the second DataFrame\n",
    "df2.show(2,False)\n",
    "\n",
    "print(f\"df2 partition size {df2.rdd.getNumPartitions()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e9c2d-5682-44dc-8025-aedb75d4881c",
   "metadata": {},
   "source": [
    "# Testing spark sql join repartition output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d53e72c-0ff6-4e42-a93a-564ecd50d4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 partitions: 4 with record count 1000\n",
      "df2 partitions: 4 with record count 1000\n",
      "joined_df.count(): 1000\n",
      "Number of partitions after join: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check the number of partitions of the original DataFrames\n",
    "print(f\"df1 partitions: {df1.rdd.getNumPartitions()} with record count {df1.count()}\")\n",
    "print(f\"df2 partitions: {df2.rdd.getNumPartitions()} with record count {df2.count()}\")\n",
    "\n",
    "# Join the two DataFrames\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # Example: Set to 4 partitions  ----- default=200\n",
    "\n",
    "joined_df = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "print(f\"joined_df.count(): {joined_df.count()}\")\n",
    "# Check the number of partitions after the join\n",
    "print(f\"Number of partitions after join: {joined_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc56e029-a586-41d3-b3fd-d11906134f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 partitions: 4\n",
      "df2 partitions: 4\n",
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  0|    1|\n",
      "|  6|    1|\n",
      "|  7|    1|\n",
      "|  9|    1|\n",
      "| 19|    1|\n",
      "| 22|    1|\n",
      "| 25|    1|\n",
      "| 26|    1|\n",
      "| 29|    1|\n",
      "| 31|    1|\n",
      "| 32|    1|\n",
      "| 34|    1|\n",
      "| 39|    1|\n",
      "| 43|    1|\n",
      "| 50|    1|\n",
      "| 54|    1|\n",
      "| 57|    1|\n",
      "| 58|    1|\n",
      "| 65|    1|\n",
      "| 68|    1|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "joined_df.groupBy(\"id\").count(): None\n",
      "Number of partitions after join: 4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import random\n",
    "\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_data(num_records):\n",
    "    data = [(i, f\"name_{i}\") for i in range(num_records)]\n",
    "    return data\n",
    "\n",
    "# Generate two datasets with 1000 records each\n",
    "data1 = generate_data(1000000)\n",
    "data2 = generate_data(1000000)\n",
    "\n",
    "# Create two DataFrames\n",
    "df1 = spark.createDataFrame(data1, [\"id\", \"name\"])  # repartition to 4 for testing\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"name\"])  # repartition to 4 for testing\n",
    "\n",
    "# Check the number of partitions of the original DataFrames\n",
    "print(f\"df1 partitions: {df1.rdd.getNumPartitions()}\")\n",
    "print(f\"df2 partitions: {df2.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Perform a join operation on the two DataFrames\n",
    "joined_df = df1.join(df2, on=\"id\")\n",
    "print(f\"\"\"joined_df.groupBy(\"id\").count(): {joined_df.groupBy(\"id\").count().show()}\"\"\")\n",
    "\n",
    "# Check the number of partitions after the join\n",
    "print(f\"Number of partitions after join: {joined_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Optionally, repartition the result if you want to enforce a specific number of partitions\n",
    "# joined_df_repartitioned = joined_df.repartition(4)  # Adjust this number based on your cluster resources\n",
    "# print(f\"Number of partitions after repartitioning: {joined_df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Show some results from the joined DataFrame\n",
    "# joined_df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2da4ea-ca0b-4573-8006-a71d615e633f",
   "metadata": {},
   "source": [
    "# Dealing with spark rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c29fe47-db10-46ce-be79-255dbdb45432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|st_id      |min(temp_value)|\n",
      "+-----------+---------------+\n",
      "|ITE00100554|-148           |\n",
      "|GM000010962|0              |\n",
      "|EZE00100082|-135           |\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the min temperature value for each station\n",
    "\n",
    "data = [\n",
    "('ITE00100554',18000101,'TMAX',-75),\n",
    "('ITE00100554',18000101,'TMIN',-148),\n",
    "('GM000010962',18000101,'PRCP',0),\n",
    "('EZE00100082',18000101,'TMAX',-86),\n",
    "('EZE00100082',18000101,'TMIN',-135)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data,[\"st_id\",\"zip\",\"temp_type\",\"temp_value\"])\n",
    "df.select(\"st_id\",\"temp_value\").groupBy(\"st_id\").agg(min(\"temp_value\")).show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eababa3-5e9c-4807-95ed-69d73af3038a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
